{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Bayesian refresher and introduction to ArviZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What does an end-to-end Bayesian workflow look like?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Refresh our understanding of Bayes' Theorem\n",
    "* Fit a small binomial model\n",
    "* Show how a full statistical workflow, even outside of Bayesian methods, requires more steps more than just model fitting\n",
    "* Introduce ArviZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "### The most common formulation\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "P(\\theta \\mid y) = \\frac{ P(y \\mid \\theta)p(\\theta)}{p(y)}\n",
    "$$\n",
    "\n",
    "This comes from a simple rearranging of terms for joint probabilities:\n",
    "\n",
    "$$\n",
    "P(\\theta, y) = P(\\theta)P(y | \\theta) = P(y)P(\\theta | y)\n",
    "$$\n",
    "\n",
    "This formula becomes interesting when we interpret $y$ as _data_ and $\\theta$ as _parameters_ for a model. \n",
    "\n",
    "### Breaking it down\n",
    "####  $P(\\theta)$ -> Prior\n",
    "_\"What is the probability of parameters given no observations\"_  \n",
    "Before we've observed any data what is a plausible probability distribution of parameters? This may come from physical constraints (temperatures are above 0 Kelvin), or domain expertise (high temperatures in Austin in summer are between 80 and 110).\n",
    "\n",
    "####  $P(y \\mid \\theta )$ -> Likelihood\n",
    "_\"What is the probability of the observed data given a model parameter value\"_  \n",
    "\n",
    "Likelihood functions tell us how \"likely\" the observed data is, for all the possible parameter values. Likelihoods perform roughly the same role as loss functions from \"machine learning\": evaluating how \"good\" of a set of model parameters are at explaining the data. Indeed, many common loss functions are derived from likelihoods.\n",
    "\n",
    "####  $P(\\theta \\mid y)$ -> Posterior distribution\n",
    "_\"What is the distribution of parameters given the observed data?\"_  \n",
    "\n",
    "After obtaining data, or making observations, what is our belief regarding the parameters of the underlying statistical model? \n",
    "\n",
    "* Estimating the posterior distribution is the goal of Bayesian analysis. \n",
    "* The process of estimating the posterior distribution often referred to as **Inference**\n",
    "* There are numerous ways to perform inference, [each with their own pros and cons](http://canyon289.github.io/pages/InferenceCheatsheet.html)\n",
    "    * In this tutorial we will only be using Markov Chain Monte Carlo (MCMC)\n",
    "\n",
    "####  $P(y)$ -> Marginal Probability of Evidence\n",
    "\n",
    "What is the probability distribution of data? In most cases this term is difficult or impossible to calculate, so much so that most inference techniques cleverly get around their calculations. MCMC is one of those techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative formulations\n",
    "\n",
    "### Likelihood notation\n",
    "I particularly like this formulation because clearly demarcates difference between Likelihood and probability terms  \n",
    "\n",
    "$$ P(\\theta | y) = \\frac{ L(\\theta | y)p(\\theta)}{p(y)} $$\n",
    "\n",
    "### Defined as a proportion\n",
    "While the posterior, likelihood, and prior are usually *distributions*, the denominator is a scalar that normalizes the numerator. In many modern Bayesian Inference Methodswe try to avoid calculating this\n",
    "\n",
    "$$ P(\\theta | y) \\propto P(y | \\theta)p(\\theta) $$\n",
    "\n",
    "### Defined with puppies\n",
    "Even if you hate math, you'd have to be a monster to hate puppies. This pictorial formula is taken from  John Kruschke's excellent book [Doing Bayesian Data Analysis](https://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884) Do note the lazy puppy on the right. The laziness is an indication of how little work this puppy does in most Bayesian Inference methods.  \n",
    "![BayesianPuppies](../../img/Doing-DBA.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
