{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.3 End-to-end statistics workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are we really doing here? If we say we are doing statistics, then from Wikipedia: \n",
    "\n",
    "> _Statistics is a branch of mathematics dealing with data collection, organization, analysis, interpretation and presentation._\n",
    "\n",
    "Notice that Bayes' Theorem is not mentioned. That's because Bayesian statistics is one methodology in the broad field of statistics/machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Versus ...\n",
    "Let's  briefly summarize the differences between Bayesian Statistics and other commonly known Statistics/Machine Learning methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian statistics versus Frequentist Statistics\n",
    "\n",
    "**Bayesians:** Given the data how how likely are my parameters? Our dataset is fixed, and our parameters have uncertainty.\n",
    "**Frequentists:** Given the parameters what is the probability of the observed data? There exist ideal parameters, and we see an example of data generated from those.\n",
    "\n",
    "Frequentist analysis also has many different estimators. A flowchart in *Statistical Rethinking* looks like the one below. Contrast this to the Bayesian workflow which appears later in this lesson.\n",
    "\n",
    "![FrequentistWorklow](../../img/FrequentistFlowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian statistics versus \"machine learning\"\n",
    "\n",
    "Machine learning can most broadly be thought as a set of algorithms which infer the \"optimal\" parameters from given data. Common examples include linear regression and neural networks. Bayesian statistics can also be classified as a machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Activity          | \"Basic\" Linear Regression                   |  Deep Learning (Classification) | Bayesian Statistics            |\n",
    "|-------------------|-------------------------------------|---------------------------------|--------------------------------|\n",
    "| Parameter Fitting | Least Squares                       | Backpropogation                 | Markov Chain Monte Carlo, ADVI |\n",
    "| Predictions       | y_hat                               | Softmax score                   | Posterior Predictive           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More than just model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When people think of machine learning often the focus is on model fitting and model \"accuracy\". A full end to end workflow requires many more activities, from model selection, model criticism, loss function design, as well as various visualizations to help interpret all of the above. This is true of all statistics methodologies, not just Bayesian Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name              | Linear Regression                   |  Deep Learning (Classification) | Bayesian Statistics            |\n",
    "|-------------------|-------------------------------------|---------------------------------|--------------------------------|\n",
    "| Parameter Fitting | Least Squares                       | Backpropogation                 | Markov Chain Monte Carlo, ADVI |\n",
    "| Metrics           | $r^2$                               | F1 Score, ROC/AUC             | WAIC, Bayes Factor             |\n",
    "| Model Parameters  | Coefficients                        | Layers, Activation Types        | Distribution Graph, Priors     |\n",
    "| Loss              | Mean Absolute Error, Squared Error | Categorical Cross Entropy       | Likelihood                     |\n",
    "| Predictions       | $\\hat{y}$                           | Softmax score                   | Posterior Predictive           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesian Landscape\n",
    "\n",
    "### Inference\n",
    "\n",
    "In our planet model we were trying to *infer* the proportion of water on a planet with only a few observations of the planet using Bayes' rule. Bayesian inference is sometimes called \"backwards probability\", since we are using observations to say something about the parameters that produced them. There are [lots of strategies](http://canyon289.github.io/pages/BayesianGlossary.html) describing these parameters!\n",
    "\n",
    "* Conjugate priors\n",
    "* Grid search\n",
    "* Variational inference (VI)\n",
    "* Markov chain Monte Carlo (MCMC)\n",
    "\n",
    "Markov chain Monte Carlo is one of the most widely used and will be the focus of the tutorial. Broadly speaking, MCMC gives you information about the posterior distribution by providing _samples_ from the posterior distribution.\n",
    "\n",
    "### Nomenclature\n",
    "\n",
    "*Monte Carlo* simulation refers to using numerical simulation to draw conclusions about a system. An example might be designing a computer program to play blackjack: the computer can spend some amount of time randomly dealing itself cards to decide on an optimal style of play.\n",
    "\n",
    "*Markov chains* are sequences where each element uses only the previous element to decide where to go. An example is training a language generator by drawing randomly from words that commonly occur after each other.\n",
    "\n",
    "*Markov chain Monte Carlo* are a class of algorithms that simulate a posterior distribution efficiently by producing samples, and using knowledge about the previous sample to make a new proposal.\n",
    "\n",
    "### Markov chain Monte Carlo methods to estimate posterior distributions\n",
    "\n",
    "To add to the mix there are numerous strategies for Markov chain Monte Carlo samplers. Some common ones are\n",
    "\n",
    "* Metropolis-Hastings\n",
    "* Gibbs\n",
    "* Slice sampling\n",
    "* Hamiltonian Monte Carlo (HMC)\n",
    "* No-U-Turn Samplers (NUTS)\n",
    "* Sequential Monte Carlo (SMC)\n",
    "\n",
    "### Probabilistic Programming Languages\n",
    "And to make things more confusing there are multiple programatic implementations of Bayesian Inference methods of all types in multiple languages\n",
    "* [PyMC3](https://github.com/pymc-devs/pymc3) in Python\n",
    "* [PyMC4](https://github.com/pymc-devs/pymc4) in Python\n",
    "* [pystan](https://github.com/stan-dev/pystan) in Python\n",
    "* [emcee](https://github.com/dfm/emcee) in Python\n",
    "* [Edward](https://github.com/blei-lab/edward) in Python\n",
    "* [Pyro](https://github.com/pyro-ppl/pyro) in Python\n",
    "* [cmdstan](https://mc-stan.org/users/interfaces/cmdstan) \n",
    "* [rainier](https://github.com/stripe/rainier) in scala\n",
    "* [Turing](https://github.com/TuringLang/Turing.jl) in julia\n",
    "* [Anglican](https://probprog.github.io/anglican/index.html) in clojure\n",
    "* [Infer.net](https://dotnet.github.io/infer/)\n",
    "* [Webppl](http://webppl.org/) in javascript\n",
    "* [Greta](https://greta-stats.org/) in r\n",
    "\n",
    "### What we will focus on\n",
    "* Python language\n",
    "* MCMC inference methods\n",
    "* PyStan and PyMC3\n",
    "\n",
    "To be very specific the full title of this tutorial would be *Introduction to Bayesian Model Evaluation, Visualization, and Comparison Using ArviZ and Markov Chain Monte Carlo inference methods, primarily  NUTS, with PyMC3 or PyStan*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End to End Bayesian Workflow\n",
    "**The aim of statistics and machine learning should not be to just fit models, but to create defensible and reproducible analysis and results**\n",
    "\n",
    "This tutorial focuses on the **evaluate and criticize the model** step of this workflow:\n",
    "\n",
    "![BayesianWorkflow](../../img/BayesianWorkflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArviZ unifies Bayesian model evaluation and criticism across inference libraries\n",
    "\n",
    "ArviZ strives to provide a unified toolset for\n",
    "\n",
    "• Diagnoses of the quality of the inference  \n",
    "• Model criticism, including evaluations of both model assumptions and model predictions  \n",
    "• Comparison of models, including model selection or model averaging  \n",
    "• Preparation of the results for a particular audience  \n",
    "\n",
    "### ArviZ currently focuses on Markov chain Monte Carlo\n",
    "\n",
    "As mentioned are [many different inference methods](http://canyon289.github.io/pages/InferenceCheatsheet.html). ArviZ is most easily used with MCMC, although the developers are hoping to extend the library to additional inference methods soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
