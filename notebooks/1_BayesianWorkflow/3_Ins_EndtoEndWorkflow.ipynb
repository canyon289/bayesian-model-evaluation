{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.3 End to End Statistics workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Wikipedia  \n",
    "_Statistics is a branch of mathematics dealing with data collection, organization, analysis, interpretation and presentation._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Versus ...\n",
    "It should be noted that Bayesian Statistics is one methodology in the broad field of statistics/machine learning. Below we briefly summarize the differences between Bayesian Statistics and other commonly known Statistics/Machine Learning methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian statistics versus Frequentist Statistics\n",
    "**Bayesians:** Given the data how how likely are my parameters?  \n",
    "**Frequentists:** Given the parameters what is the probability of the observed data?\n",
    "\n",
    "Frequentist analysis also has many different estimators. A flowchart in *Statistical Rethinking* looks like the one below. Contrast this to the Bayesian workflow which appears later in this lesson.\n",
    "\n",
    "![FrequentistWorklow](../../img/FrequentistFlowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian statistics versus \"Machine Learning\"\n",
    "Machine learning can most broadly be thought as a set of algorithms which infer their \"optimal\" parameters from given data. Common examples include linear regressions, or neural networks. Bayesian Statistics can also be classified as a machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Activity          | \"Basic\" Linear Regression                   |  Deep Learning (Classification) | Bayesian Statistics            |\n",
    "|-------------------|-------------------------------------|---------------------------------|--------------------------------|\n",
    "| Parameter Fitting | Least Squares                       | Backpropogation                 | Markov Chain Monte Carlo, ADVI |\n",
    "| Predictions       | y_hat                               | Softmax score                   | Posterior Predictive           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More than just model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When people think of machine learning often the focus is on model fitting and model \"accuracy\". A full end to end workflow requires many more activities, from model selection, model criticism, loss function design, as well as various visualizations to help interpret all of the above. This is true of all statistics methodologies, not just Bayesian Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name              | Linear Regression                   |  Deep Learning (Classification) | Bayesian Statistics            |\n",
    "|-------------------|-------------------------------------|---------------------------------|--------------------------------|\n",
    "| Parameter Fitting | Least Squares                       | Backpropogation                 | Markov Chain Monte Carlo, ADVI |\n",
    "| Metrics           | $r^2$                               | F1 Score, ROC/AUC             | WAIC, Bayes Factor             |\n",
    "| Model Parameters  | Coefficients                        | Layers, Activation Types        | Distribution Graph, Priors     |\n",
    "| Loss              | Mean Absolute Error, Squared Error | Categorical Cross Entropy       | Likelihood                     |\n",
    "| Predictions       | $\\hat{y}$                           | Softmax score                   | Posterior Predictive           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesian Landscape\n",
    "\n",
    "### Inference\n",
    "In our planet model we were trying to *infer* the proportion of water on a planet with only a few observations of the planet using Bayes Rule. More specifically we were making estimates by sampling the posterior distribution using a **specific** inference method called Markov Chain Monte Carlo. \n",
    "\n",
    "This is important because there are [many different inference methods](http://canyon289.github.io/pages/BayesianGlossary.html). A short list includes\n",
    "* Conjugate Priors\n",
    "* Grid Search\n",
    "* Variational Inference\n",
    "* Markov Chain Monte Carlo\n",
    "\n",
    "At time of writing however Markov Chain Monte Carlo is one of the most widely used and will be the focus of the tutorial.\n",
    "\n",
    "### Markov Chain Monte Carlo methods to estimate posterior distributions\n",
    "To add to the mix there are numerous Markov Chain Monte Carlo samplers. A short list of common ones are\n",
    "* Metropolis Hasting\n",
    "* Hamiltonian Monte Carlo (HMC)\n",
    "* No U Turn Samplers (NUTS)\n",
    "* Sequential Monte Carlo (SMC)\n",
    "\n",
    "### Probabilistic Programming Languages\n",
    "And to make things more confusing there are multiple programatic implementations of Bayesian Inference methods of all types in multiple languages\n",
    "* PyMC3 in Python\n",
    "* PyMC4 in Python\n",
    "* pystan in Python\n",
    "* emcee in Python\n",
    "* Edward in Python\n",
    "* Pyro in Python\n",
    "* cmdstan \n",
    "* [rainier](https://github.com/stripe/rainier) in scala\n",
    "* [Turing](https://github.com/TuringLang/Turing.jl) in julia\n",
    "* [Anglican](https://probprog.github.io/anglican/index.html) in clojure\n",
    "* [Infer.net](https://dotnet.github.io/infer/)\n",
    "* [Webppl](http://webppl.org/) in javascript\n",
    "* [Greta](https://greta-stats.org/) in r\n",
    "\n",
    "### What we will focus on\n",
    "* Python language\n",
    "* MCMC Inference Methods\n",
    "* PyStan and PyMC3\n",
    "\n",
    "To be very specific the full title of this tutorial would be *Introduction to Bayesian Model Evaluation, Visualization, and Comparison Using Arviz and Markov Chain Monte Carlo inference methods, primarily  NUTS, with PyMC3 or pystan*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End to End Bayesian Workflow\n",
    "**The aim of statistics and machine learning should not be to just fit models, but to create defendable and reproducible analysis and results**\n",
    "\n",
    "This tutorial focuses on how to use ArviZ to do just that using all steps of this workflow.\n",
    "\n",
    "![BayesianWorkflow](../../img/BayesianWorkflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArviZ unifies of Bayesian Model evaluation and criticism across inference libraries\n",
    "`\n",
    "ArviZ strive to provide a unified toolset for\n",
    "\n",
    "• Diagnoses of the quality of the inference\n",
    "• Model criticism, including evaluations of both model assumptions and model predictions\n",
    "• Comparison of models, including model selection or model averaging\n",
    "• Preparation of the results for a particular audience\n",
    "\n",
    "### ArviZ currently focuses on Markov Chain Monte Carlo\n",
    "As mentioned are [many different inference methods](http://canyon289.github.io/pages/BayesianGlossary.html). As is such ArviZ is most easily used with that inference method, although the core developers are hoping to extend the library to additional inference methods soon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
